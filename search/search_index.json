{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Object Tracking using Yolov8 and FMCW Radar","text":"<p>This project is currently in progress. </p> <p>This project's goal is to create software that can be used to track objects using both live radar sensor data and video using yolo. The latest version of this project is using Yolov8 for object detection through video. The radar processing is done using a IMST custom radar toolkit, with custom implemented signal and CFAR processing. The tracking portion of those individual signal results is done using Stone Soup's library.</p>"},{"location":"#where-to-start","title":"Where to Start","text":"<ul> <li>Get a quickstart by running this software in a docker container - Running in Docker</li> <li>Customizing the arguments run in the container - Configuration</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Please see the Developer Environment section under setup.</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The design of this process uses python multiprocess queues to move data between the radar and video processing to the tracking algorithm.  The queue allows for better synchronization of the data, and abstraction to swap out various video or tracking algorithms in the future. The queue follow the data shape described in Detections Spec</p> <pre><code>flowchart LR\n    %% Class definition to allow for bigger icon pictures\n    classDef bigNode font-size:24px;\n\n    subgraph tracking[\"Tracking.py\"]\n        %%  Radar Processing SubGraph\n        subgraph radarProc[\"**RadarProcess.py**\"]\n            style desc1 text-align:left\n            desc1[\"`1 Collect data\n            2 Data drocessing\n            3 Put detection into queue`\"]\n        end\n\n        %%  Image Processing SubGraph\n        subgraph imageProc[\"`**ImageProcess.py**`\"]\n            style desc2 text-align:left\n            desc2[\"`1 Collect image\n            2 Data processing\n            3 Put detection into queue`\"]\n        end\n        Queue[[\"MP Queue\"]]\n        subgraph trackProc[\"`**ObjectTracking.py**`\"]\n            style desc3 text-align:left\n            desc3[\"`1 Read data from queue\n            2 Update tracks based of new data\n            3 Report current tracks`\"]\n        end\n    end\n    Radar[[\"Radar\"]]:::bigNode\n    Video[[Video]]:::bigNode\n\n    Radar &lt;--&gt; radarProc\n    Video &lt;--&gt; imageProc\n    radarProc --&gt; Queue\n    imageProc --&gt; Queue\n    Queue -- Data read from Queue --&gt; trackProc</code></pre>"},{"location":"#algorithm-for-data-synchronization","title":"Algorithm for Data Synchronization","text":"<p>The system processes data from both radar and video sensors, each sending data to the same queues. Data from both sensors can arrive at different rates, so a batching mechanism with a configurable synchronization periods is used to pull data from the queue.</p> <p>Every x seconds, the time for the configurable syncrhonization period, all available data is retrieved from the queue. This window is chosen because it ensures that both radar and video processing have completed at least once in that period. If both radar and video data are available within the same batch, they are combined and used for tracking. If only one is available, it is used alone.</p> <p>When multiple detections are present within a synchronization period, the latest detection is used for tracking. Future improvements will allow either taking an average of the detections or selecting the detection with the highest confidence score for better tracking accuracy.</p> <p>As performance improves, the batching window can be adjusted to handle data more frequently while maintaining synchronization between the radar and video data.</p> <pre><code>flowchart TD\n\u00a0subgraph subGraph0[\"Video Process\"]\n\u00a0 \u00a0 \u00a0 \u00a0 A1(\"Video Data Capture\")\n\u00a0 \u00a0 \u00a0 \u00a0 Y1(\"Video Processing\")\n\u00a0 \u00a0 \u00a0 \u00a0 Q1[\"*detect_queue*\"]\n\u00a0 end\n\u00a0subgraph subGraph1[\"Radar Process\"]\n\u00a0 \u00a0 \u00a0 \u00a0 B1(\"Radar Data Capture\")\n\u00a0 \u00a0 \u00a0 \u00a0 Z1[\"Radar Processing\"]\n\u00a0 \u00a0 \u00a0 \u00a0 Q2[\"*detect_queue*\"]\n\u00a0 end\n\u00a0subgraph subGraph2[\"Main Processing Loop\"]\n\u00a0 \u00a0 \u00a0 \u00a0 P1[\"Check *detect_queue*\"]\n\u00a0 \u00a0 \u00a0 \u00a0 D1(\"Within *period*?\")\n\u00a0 \u00a0 \u00a0 \u00a0 I1[\"Check if Radar or Video Detection. &lt;br&gt; Complete any specific post-processing including normalization if required.\"]\n\u00a0 \u00a0 \u00a0 \u00a0 I2[\"Buffer Image Data for Next *sync period*\"]\n\u00a0 \u00a0 \u00a0 \u00a0 T1[\"Update Current Tracks\"]\n\u00a0 end\n\u00a0 \u00a0 A1 -- Send Frame --&gt; Y1\n\u00a0 \u00a0 Y1 -- Push Detection --&gt; Q1\n\u00a0 \u00a0 B1 -- Sends to --&gt; Z1\n\u00a0 \u00a0 Z1 -- Push Detections --&gt; Q2\n\u00a0 \u00a0 Q1 --&gt; P1\n\u00a0 \u00a0 Q2 --&gt; P1\n\u00a0 \u00a0 P1 -- Check timestamps within *sync period* --&gt; D1\n\u00a0 \u00a0 D1 -- Yes --&gt; I1\n\u00a0 \u00a0 D1 -- No --&gt; I2\n\u00a0 \u00a0 I1 -- Pass Detections --&gt; T1\n\u00a0 \u00a0 I2 -- Use buffered data in next loop --&gt; P1</code></pre>"},{"location":"devNotes/","title":"devNotes","text":"<p>Scratch pad of commands for various issues. <pre><code>export QT_QPA_PLATFORM=offscreen\nsudo apt-get install python3-tk\n\n# Debugging\npip uninstall opencv-python\npip install opencv-python-headless\n# Fix issues\nexport QT_QPA_PLATFORM=offscreen\napt install libxcb-cursor0 # / Maybe??\n\npython tracking.py --radar-only\npython tracking.py --video-only\npython tracking.py --show-plots\n</code></pre></p>"},{"location":"configuration/cliArguments/","title":"CLI Arguments","text":""},{"location":"configuration/cliArguments/#cli-arguments-for-trackingpy","title":"CLI Arguments for Tracking.py","text":"<p>By default, the Tracking.py program will use the options that are passed through the configuration files. See Configuration for more details.</p> <p>However, for further customizations the there are various options that can be passed to the <code>tracking.py</code> program when running from the cli. The arguments passed through the CLI will take priority over the configuration files. </p> <p>A full list can be seen by running <code>python3 tracking.py --help</code> from an environment with python and this project cloned. All options are also described below.</p> <pre><code>--output-folder,        # Output folder for all results to be saved to \n--radar-start-delay     # Delay in seconds before starting the radar after the video processing starts\n\n# Options for disabling parts of the program, but default video and radar tracking are enabled. Plots are not.\n--skip-video           # skip video based tracking\n--skip-radar           # skip radar based tracking\n--skip-tracking        # skip tracking algorithm and path determination\n--enable-plot          # enable plotting\n\n# Options for the radar configuration\n--radar-config          #radar configuration file path\n--radar-ip              # ip address of the radar, DEFAULT - 192.168.0.2\n--disable-record        # option to disable recording the radar data\n--radar-record-path     # path to folder to record radar data to, DEFAULT - /output\n--radar-from-file       # use previously recorded radar data\n--radar-source          # path to folder to read prerecorded radar data from. Only used if \"--radar-rerun\" is set\n--radar-disable-print   # disable printing of radar params\n\n# Options for the video configuration\n--video-config          # video configuration file path\n--model-weights         #model weights path. yolo wil attempt to download the weight if the arg is not a path\n--video-source          #source of the video data, supports all yolo default options\n--disable-save-video    #disable saving the video data\n--show-live-video       # if a live-feed of the video should be shown while running \n\n# Options for the tracking configuration\n--tracking-config       # tracking configuration file path, DEFAULT - /configuration/TrackingConfig.yaml\n--show-tracking-plot    # show the tracking plot on completion\n--tracking-disable-save # disable saving the tracking data\n--batching-time         # time to accumulate detections before sending to tracking algorithm - default 0.3 seconds\n</code></pre>"},{"location":"configuration/configuration/","title":"Configuration Files","text":""},{"location":"configuration/configuration/#radar-configuration-file","title":"Radar Configuration File","text":"<p>The radar configuration can be found in the RadarConfig.yaml file. </p> RadarConfig.yaml <pre><code># Radar signal configuration - for now only FMCW Up-Ramp is supported\nminimumFrequencyMhz: 24000\nmaximumFrequencyMhz: 24750\nrampTimeFmcwChirp: 1\n# Tx attenuation during measurement\n# 0 ~ 0dB, 1 ~ 0.4dB, 2 ~ 0.8dB, 3 ~ 1.4 dB, 4 ~ 2.5dB, 5 ~ 4dB, 6 ~6dB, 7 ~ 9dB\nattenuation: 0\n\n# Radar connection configuration\nethernetParams:\nethernetPort: 1024\nethernetIp: \"192.168.0.2\" # IP to connect to the radar withs\n\n# Radar signal processing configuration\ncfarParams:\ncfarNumGuard: 5\ncfarNumTrainingCells: 10\ncfarThreshold: 10\ncfarType: \"CASO\" # Options are CASO, LEADING_EDGE\n\n# Run configuration\nrun: LIVE # Options are LIVE, RERUN\nsourcePath: \"/data/radar/run2-TD/\" # Path to the data to be processed for RERUN mode\n\nrecordData: True # Record radar data to disk\nrecordDataPath: \"/output\" # Path to the data to be recorded\n\n# How many Radar results to keep in the buffer for analysis and CFAR \nprocessingWindow: 200\n\n# Print radar runtime settings to console on startup\nprintSettings: True\n</code></pre>"},{"location":"configuration/configuration/#video-processing-configuration-file-yolo-params","title":"Video Processing Configuration File (Yolo Params)","text":"<p>The radar configuration can be found in the YoloConfig.yaml file. </p> VideoConfig.yaml <pre><code># Configuration for the Yolo Processing\nmodelWeights: \"./yolov8n.pt\" # If a path on disk is specified, it will not be downloaded by Yolo\nsaveRawImages: True # Determine if the raw images should be saved\nsaveProcessedVideo: True\noutputDirectory: \"/output\"\nvideoSource: \"https://youtu.be/LNwODJXcvt4\"\n# videoSource: \"/data/video/M0101.mp4\"\nconfidenceThreshold: 0.3\niouThreshold: 0.5\nvideoStream: True\n\n# Configuration for showing the Live Video During Yolo Processing\nshowBoxesInVideo: True\nshowVideo: False\nprintDetectedObjects: False # Print the detected objects in the console as noticed\n\n# Camera Details\ncameraHorizontalFOV: 170\ncameraZoomFactor: 1.0\nimageWidth: 640\nimageHeight: 352\n\nvideoDelayBetweenProcessingSec: 0.1\n</code></pre>"},{"location":"configuration/configuration/#tracking-configuration-file","title":"Tracking Configuration File","text":"TrackingConfig.yaml <pre><code># Filter Configuration for different tracking algorithms\nactiveFilter: gmPHD\nfilters:\n# GM PHD \ngmPHD:\n    birthCovariance: 200 # covariance of the birth state in a distance of meters - by default this will set to the maximum resolution range of the sensors!\n    expectedVelocity: 1 # expected velocity of the tracked object in meters per second\n    noiseCovarianceDistance: 1 # covariance of the noise in a distance of meters\n    defaultCovarianceDistance: 1 # default covariance of the tracked object in a distance of meters\n    defaultConvarianceVelocity: 0.3 # default covariance of the tracked object in a velocity of meters per second\n    probabilityOfDetection: 0.8 # probability of detection\n    probabilityOfDeath: 0.01 # probability of death\n    clusterRate: 7.0\n\n    mergeThreshold: 5 # Threshold Squared Mahalanobis distance\n    pruneThreshold: 0.00000001  # Threshold component weight i.e. 1e-8\n    stateThreshold: 0.25\n\n# Detection clustering configuration\nminDetectionsToCluster: 1\nmaxDistanceBetweenClusteredObjectsM: 2\n\n# Tracking plot configuration \ntrackTailLength: 0.1 # O to 1, 0 means no tail, 1 means full tail\n\n# Configuration for processing and memory management\nmaxTrackQueueSize: 200\n\n# Show the Stone Soup tracking plot when the program exits\nshowTrackingPlot: False\n\n# Output path for the tracking results\nsaveTrackingResults: True\noutputDirectory: '/output'\n</code></pre>"},{"location":"configuration/detections-spec/","title":"Detections","text":"<p>Detections from both Radar and Video will be put into the following central format.  This is the datashape the ObjectTracking.py expects the data to be in, when reading from the MP Queue for object tracking</p> <p>This will allow them to both be used by StoneSoup's tracking algorithms. This also offers abstraction, incase later the radar or video processing algorithm changes. As long as this format is used, the different segments can be swapped out.</p> <pre><code>{\n    \"timestamp\": \"&lt;some-datetime&gt;\",\n    \"type\": \"radar | video\",\n    \"detections\": [\n            {\n                \"object\": \"&lt;typeOfObject1&gt;\",\n                \"detection\":  [x1, x_v1, y, y_v1]\n            },\n            {\n                \"object\": \"&lt;typeOfObject2&gt;\",\n                \"detection\":  [x2, x_v2, y, y_v2]\n            } \n        ]\n}\n</code></pre>"},{"location":"configuration/imageDetails/","title":"Docker Image Details","text":"<p>The Ultralytics docker image was used as the base image for this project. It includes the requried GPU tools to run yolo and was easy to modify, so it was used as a base image. It also offers base images for the Jetson-5 board which is ideal for our purposes.</p> <p>The configuration, data, output and tracking directories are added to the image after it's built for this project. Each of those sections is described in below.</p> <ul> <li> <p>configuration - Directory that contains the required configuration files for the application to run. Many settings can also be overwritten through the cli interface.</p> </li> <li> <p>data - Directory recomended for the user to mount pre-recorded data/samples to run the tracking algorithm against. This is configurable, but this is shown as a placeholder.</p> </li> <li> <p>output - Output directory where files from tracking will be saved, it is recommeded to mount this directory to a local volume to grab the data easily.</p> </li> <li> <p>tracking - directory that contains the additional radar tracking and object tracking pipeline</p> </li> </ul>"},{"location":"configuration/imageDetails/#container-structure-after-additions","title":"Container Structure After Additions","text":"<pre>\n|-- bin/\n|-- boot/\n|-- configuration/\n|   |-- RadarConfig.yaml\n|   |-- VideoConfig.yaml\n|   |-- TrackingConfig.yaml\n|-- data/\n|   |-- radar\n|   |-- video\n|-- dev/\n|-- etc/\n|-- home/\n|-- media/\n|-- output\n|   |-- 2024-09-15_21-45-01\n|   |-- other-outputs\n|-- tracking/\n|   |-- UAV-Object-Tracking/\n|   |   |-- tracking.py\n|   |   |-- PROJECT FILES\n|-- ultralytics/\n|   |-- CITATION.cff\n|   |-- CONTRIBUTING.md\n|   |-- LICENSE\n|   |-- README.md\n|   |-- README.zh-CN.md\n|   |-- calibration_image_sample_data_20x128x128x3_float32.npy\n|   |-- docker/\n|   |-- docs/\n|   |-- examples/\n|   |-- mkdocs.yml\n|   |-- pyproject.toml\n|   |-- tests/\n|   |-- ultralytics/\n|   |-- ultralytics.egg-info\n|   `-- yolov8n.pt\n|-- var/\n| other system folders\n</pre>"},{"location":"guides/runningInDocker/","title":"Running In Docker","text":"<p>This project publishes docker images to DockerHub to allow users to quickly run the software. Two variants of the images are published, for linux and for Jetson5 devices. For more details on the published images please see the Image Details page.</p> <p>To get started with the latest images, please pull it onto your computer. </p>"},{"location":"guides/runningInDocker/#pulling-the-images","title":"Pulling the Images","text":"LinuxJetson5 <pre><code>docker pull nbowness/uav-experiments:latest\n</code></pre> <pre><code>docker pull nbowness/uav-experiments:latest-jetson-jetpack5\n</code></pre>"},{"location":"guides/runningInDocker/#tag-images","title":"Tag Images","text":"<p>To simplify future command, you can tag the images to a shorter name. All commands shown below will use the short form <code>tracking-image</code>.</p> LinuxJetson5 <pre><code>docker tag nbowness/uav-experiments:latest tracking-image\n</code></pre> <pre><code>docker tag nbowness/uav-experiments:latest-jetson-jetpack5 tracking-image\n</code></pre>"},{"location":"guides/runningInDocker/#running-the-container","title":"Running the Container","text":"<p>To run the container interactively, you can run the following command to launch the container. To run, non-interactively remove the <code>-it</code> option. </p> LinuxJetson5 <pre><code>docker run -it tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image\n</code></pre>"},{"location":"guides/runningInDocker/#running-the-tracking-software","title":"Running the Tracking Software","text":"<p>After launching the container on either platform, you can run the following command to start the program. Use this default configuration, the radar and video process will run on \"live\" data, until told to exit with <code>q+ENTER</code> (or <code>CTRL+c</code>).</p> <pre><code>python3 tracking.py \n</code></pre>"},{"location":"guides/runningInDocker/#saving-data-output-locally","title":"Saving Data Output Locally","text":"<p>You can save all output of the container to a local folder, by mounting a local folder on disk to the <code>/output</code>directory of the container. The below command will mount a local output folder to the container, feel free to modify it.</p> LinuxJetson5 <pre><code>docker run -it -v \"$(pwd)\"/output:/output tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it -v \"$(pwd)\"/output:/output tracking-image \n</code></pre>"},{"location":"guides/runningInDocker/#configuration","title":"Configuration","text":"<p>By default the above commands will run the program with default configuration. This will use the default configuration files, for more details please review the configuration page.</p> <p>To change the configuration, there are 2 options 1. Change options with CLI arguments when running <code>tracking.py</code>.  1. Mount configuration files to the container with updated arguments.</p>"},{"location":"guides/runningInDocker/#cli-arguments-for-program","title":"CLI Arguments for Program","text":"<p>The are numerous CLI arguments for the <code>tracking.py</code> program. To see all options you can run the <code>--help</code> command. </p> <pre><code>python3 tracking.py --help \n</code></pre> <p>An example of updating the Radar's IP address can be see below.</p> <pre><code>python3 tracking.py --radar-ip 10.0.1.60\n</code></pre>"},{"location":"guides/runningInDocker/#mounting-configuration-files","title":"Mounting Configuration Files","text":"<p>To mount configuration files that you can change locally, and will get used by the container you can mount a volume to the <code>/configuration</code> directory in the container.</p> LinuxJetson5 <pre><code>docker run -it -v \"$(pwd)\"/configuration:/configuration tracking-image \n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it -v \"$(pwd)\"/configuration:/configuration tracking-image\n</code></pre>"},{"location":"guides/runningInDocker/#running-with-different-configurations-samples","title":"Running with Different Configurations - Samples","text":"<ul> <li>Running the container on previously collected data - details</li> <li>Running the container with only radar processing - details</li> <li>Running the container with only video processing - details</li> <li>Running the container with displayed results - datails</li> </ul>"},{"location":"guides/runningOnCollectedData/","title":"Running on previously collected data","text":"<p>To run the <code>tracking.py</code> program on previously recorded data, there are 2 options:</p> <ol> <li>Pass the CLI arguments to the program with required details.</li> <li>Mount configuration files for Radar and/or Video processing with required details.</li> </ol> <p>Note</p> <p>All options below, will assume you have pulled the docker image, and tagged it as <code>tracking-image</code> to simplify the following commands (so they're not platform dependant). Please see the docker quickstart section for more details.</p>"},{"location":"guides/runningOnCollectedData/#radar","title":"Radar","text":""},{"location":"guides/runningOnCollectedData/#using-cli-arguments-to-run-on-collected-data","title":"Using CLI Arguments to Run on Collected Data","text":"<p>To run on collected data by changing CLI arguments, you can do that following.</p> <ol> <li> <p>Run Container with Data Mount     To access data within the container you'll need to mount the local data into it. This example mounts the pwd/data folder into the /data location in the container. These can both be configured as you'd like.</p> LinuxJetson5 <pre><code>docker run -it tracking-image -v \"$(pwd)\"/data:/data\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image -v \"$(pwd)\"/data:/data\n</code></pre> </li> <li> <p>Run the tracking program, specify the location to the data</p> <pre><code>python3 tracking.py --radar-from-file --radar-from-file &lt;path-to-file&gt;\n</code></pre> </li> </ol>"},{"location":"guides/runningOnCollectedData/#mounting-configuration-files","title":"Mounting Configuration Files","text":"<p>Mount the RadarConfig.yaml file to the <code>/configuration</code> section of the container when running. You may also want to mount <code>/data</code> to the container to easily pass data into it.</p> <ol> <li> <p>Running Container with Configuration/Data Mounts</p> LinuxJetson5 <pre><code>docker run -it tracking-image -v \"$(pwd)\"/configuration:/configuration -v \"$(pwd)\"/data:/data\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image -v \"$(pwd)\"/configuration:/configuration -v \"$(pwd)\"/data:/data\n</code></pre> </li> <li> <p>Modify these 2 settings in the <code>RadarConfig.yaml</code> configuration file</p> <pre><code># Run configuration\nrun: RERUN # Changed to RERUN\nsourcePath: \"/data/radar/run2-TD/\" # Path to the data to be processed for RERUN mode\n</code></pre> <p>For full details on the RadarConfig.yaml file please see the configuration section.</p> </li> <li> <p>Run the tracking program, specify the configuration file location. By default it's set to <code>/configuration/RadarConfig.yaml</code>.</p> <pre><code>python3 tracking.py --radar-config &lt;path-to-config-file&gt;\n</code></pre> </li> </ol>"},{"location":"guides/runningOnCollectedData/#video","title":"Video","text":""},{"location":"guides/runningOnCollectedData/#cli-arguments-to-run-on-collected-data","title":"CLI Arguments to Run on Collected Data","text":"<ol> <li> <p>Run Container with Data Mount     To access data within the container you'll need to mount the local data into it. This example mounts the pwd/data folder into the /data location in the container. These can both be configured as you'd like.</p> LinuxJetson5 <pre><code>docker run -it tracking-image -v \"$(pwd)\"/data:/data\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image -v \"$(pwd)\"/data:/data\n</code></pre> </li> <li> <p>Run the tracking program, specify the location to the data</p> <pre><code>python3 tracking.py --video-source &lt;path-to-file&gt;\n</code></pre> </li> </ol>"},{"location":"guides/runningOnCollectedData/#mounting-configuration-files_1","title":"Mounting Configuration Files","text":"<p>Mount the VideoConfig.yaml file to the <code>/configuration</code> section of the container when running. You may also want to mount <code>/data</code> to the container to easily pass data into it.</p> <ol> <li> <p>Running Container with Configuration/Data Mounts</p> LinuxJetson5 <pre><code>docker run -it tracking-image -v \"$(pwd)\"/configuration:/configuration -v \"$(pwd)\"/data:/data\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image -v \"$(pwd)\"/configuration:/configuration -v \"$(pwd)\"/data:/data\n</code></pre> </li> <li> <p>Modify these 2 settings in the <code>VideoConfig.yaml</code> configuration file</p> <pre><code># Yolo Options\nvideoSource: \"&lt;absolutePathToFileOnDisk&gt;\"\n</code></pre> <p>For full details on the RadarConfig.yaml file please see the configuration section.</p> </li> <li> <p>Running the tracking command.     Optionally, if you mounted the configuration at a different path you can specify that path too. By default it's set to <code>/configuration/VideoConfig.yaml</code>.     <pre><code>python3 tracking.py --video-config &lt;my-data-path&gt;\n</code></pre></p> </li> </ol>"},{"location":"guides/runningTheRadar/","title":"Radar Processing","text":"<p>Note</p> <p>All options below, will assume you have pulled the docker image, and tagged it as <code>tracking-image</code> to simplify the following commands (so they're not platform dependant). Please see the docker quickstart section for more details.</p> <p>To run only the Radar processing section of the <code>tracking</code> program, you can disable video processing. This can be beneficial if you only want to collect radar data, or do not have a video source available.</p> <p>This will walk you through how to modify the container running tracking, but also applied to the local developer environment as well.</p>"},{"location":"guides/runningTheRadar/#running-only-radar-processing","title":"Running ONLY Radar Processing","text":"<p>This configuration is useful if you'd only like to collect Radar data, and are not worried about tracking anything. This is useful for testing radar data collection.</p> <ol> <li> <p>Run the Container Interactively</p> LinuxJetson5 <pre><code>docker run -it tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image\n</code></pre> </li> <li> <p>Run the tracking program, with video processing and radar tracking disabled. <pre><code>python3 tracking.py --skip-video --skip-tracking\n</code></pre></p> </li> </ol>"},{"location":"guides/runningTheRadar/#running-with-radar-and-tracking-processing","title":"Running with Radar and Tracking Processing","text":"<p>This configuration is useful if you'd only like to collect Radar data and see how it influences the tracking.</p> <ol> <li> <p>Run the Container Interactively</p> LinuxJetson5 <pre><code>docker run -it tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image\n</code></pre> </li> <li> <p>Run the tracking program, with video processing and radar tracking disabled. <pre><code>python3 tracking.py --skip-video\n</code></pre></p> </li> </ol>"},{"location":"guides/runningTheRadar/#disable-saving-radar-data","title":"Disable Saving Radar Data","text":"<p>When running the tracking program, include the <code>--disable-record</code>. <pre><code>python3 tracking.py --disable-record\n</code></pre></p>"},{"location":"guides/runningVideoProcessing/","title":"Video Processing","text":"<p>Note</p> <p>All options below, will assume you have pulled the docker image, and tagged it as <code>tracking-image</code> to simplify the following commands (so they're not platform dependant). Please see the docker quickstart section for more details.</p> <p>To run only the Radar processing section of the <code>tracking</code> program, you can disable video processing. This can be beneficial if you only want to collect radar data, or do not have a video source available.</p> <p>This will walk you through how to modify the container running tracking, but also applied to the local developer environment as well.</p>"},{"location":"guides/runningVideoProcessing/#running-only-video-processing","title":"Running ONLY Video Processing","text":"<p>This configuration is useful if you'd only like to collect video data, and are not worried about tracking anything. This is useful for testing video data collection and ensure the model is running. It's also useful to see the Yolo performance by itself</p> <ol> <li>Run the Container Interactively</li> </ol> LinuxJetson5 <pre><code>docker run -it tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it\n</code></pre> <ol> <li>Run the tracking program, with radar processing and all tracking disabled. <pre><code>python3 tracking.py --skip-radar --skip-tracking\n</code></pre></li> </ol>"},{"location":"guides/runningVideoProcessing/#running-with-video-and-tracking-processing","title":"Running with Video and Tracking Processing","text":"<p>This configuration is useful if you'd only like to collect video data and see how it influences the tracking.</p> <ol> <li> <p>Run the Container Interactively</p> LinuxJetson5 <pre><code>docker run -it tracking-image\n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it tracking-image\n</code></pre> </li> <li> <p>Run the tracking program, with video processing and radar tracking disabled using default configuration. <pre><code>python3 tracking.py --skip-radar\n</code></pre></p> </li> </ol>"},{"location":"guides/runningVideoProcessing/#disable-saving-video-data","title":"Disable Saving Video Data","text":"<p>When running the tracking program, include the <code>--disable-save-video</code> option. <pre><code>python3 tracking.py --disable-save-video\n</code></pre></p>"},{"location":"guides/runningVideoProcessing/#runing-with-a-local-camera","title":"Runing with a Local Camera","text":"<ol> <li> <p>Run the Container Interactively</p> LinuxJetson5 <pre><code>docker run -it -v \"$(pwd)/output:/output\" --device=/dev/video0:/dev/video0 tracking-image \n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it -v \"$(pwd)/output:/output\" --device=/dev/video0:/dev/video0 tracking-image\n</code></pre> </li> <li> <p>Run the tracking program with a local device, specified by the integer. This will skip the radar processing, but will include the tracking.</p> </li> </ol> <pre><code>python3 tracking.py --skip-radar --video-source \"0\"\n</code></pre>"},{"location":"guides/runningWithVisuals/","title":"Running the Container with Visuals","text":"<p>These options should let you run the container with visuals enabled, meaning you can see the Yolo results live. As well on completion, the stone-soup graph can be shown to the user. Assuming they have a browser enabled.</p> <ol> <li> <p>Run the Container Interactively</p> <p>To run the container interactively with visuals, you will need these additional options added to the normal run command.</p> <p>Note: This has only be tested on linux, there may be different arguments needed for Jetson. <pre><code>xhost +local:docker &amp;&amp; docker run -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -it tracking-image\n</code></pre></p> </li> <li> <p>Run the tracking program, with video processing and radar tracking disabled. <pre><code>python3 tracking.py --show-tracking-plot -show-video-live\n</code></pre></p> </li> </ol>"},{"location":"setup/calibratingBBCoeff/","title":"Calibrating the BB Coefficients","text":"<p>So currently the distance calculations for objects relative to the camera must be calibrated. For best results, please individually calibrate the BB coefficient for you individual objects.  For convience, there are some helpful functions to help calibrate a collection of objects after just calibrating from one known one -- determine_bb_coeffs.py</p>"},{"location":"setup/calibratingBBCoeff/#steps-to-calibrate","title":"Steps to Calibrate","text":"<ol> <li>Determine your camera's details, including horizon FOV, default zoom factor, and the videos width and height in pixels. </li> <li>Measure the width of an object.</li> <li>Do the following at a few different distances from the camera to ensure the estimation is accurate. The coefficient should be mostly constant at different distances.</li> <li>Measure the distance of an object from the camera.</li> <li>Call the following function <code>determine_bb_coeff_for_object(...)</code> using your IDE, or the following cli command. It will use the following calcualte </li> </ol> <p>Bounding Box (BB) Coefficient Calculation:</p> \\[ \\text{BB Coefficient} = \\frac{\\text{focal length (pixels)} \\times \\text{object width (meters)}}{\\text{distance from camera (meters)} \\times \\text{object pixel width (pixels)}} \\] <pre><code># Example: python3 -c \"import video/determine_bb_coeffs; determine_bb_coeffs.determine_bb_coeff_for_object(distance_from_camera, object_name, object_width_meters, camera_horizontal_fov, image_width, image_height)\"\ncd video\npython3 -c \"import determine_bb_coeffs; determine_bb_coeffs.determine_bb_coeff_for_object(10, 'person', 0.52, 65, 1920, 1080, 'test.jpg')\"\n</code></pre> <ol> <li>Use the determined BB coefficient, to populate the coefficients for lots of related objects. The formula can be see below.This uses the approximate width of the object you calibrated against versus the other object's width to approximate their coefficients as well.</li> </ol> <p>Bounding Box (BB) Coefficient Proportion Calculation:</p> \\[ \\text{BB Coefficient}_{\\text{object}} = \\text{BB Coefficient}_{\\text{known}} \\times \\frac{\\text{size of known object}}{\\text{size of other object}} \\] <p>Call this function with the determines BB coefficients for lots of related objects based off the correct one you just found. </p> <p><pre><code># Example: python3 -c \"import determine_bb_coeffs; determine_bb_coeffs.determine_all_bb_coefficients(object_name, object_known_width, known_bb_coeff, \"BBCoefficients.yaml\")\n\npython3 -c \"import determine_bb_coeffs; determine_bb_coeffs.determine_all_bb_coefficients('person', 0.52, 0.127383, \"BBCoefficients.yaml\")\"\n</code></pre> 5. That function should have created a new list of coefficients. Please use that for future tracking, and add it to the configuration file.</p> <p>Note: Alterative is to edit the bottom of the determine_bb_coeffs.py, which will create the BBCoefficients.yaml file for you when you run it. Just update the values to your desired ones.</p>"},{"location":"setup/devEnvironment/","title":"Development Requirements","text":"<p>The development requirements for this project are as follows: 1. Python 3.10 2. Pip 3. (Optional) The Nvidia toolkit installed so a GPU can be used and accessed.</p>"},{"location":"setup/devEnvironment/#running-using-local-python-environment","title":"Running Using Local Python Environment","text":"<ul> <li>Install the required python version</li> <li>Install pip if you don't already have it</li> <li>Setup a virtual environemtn for this project</li> <li>Install all dependencies using <code>pip install -r requirements.txt</code></li> </ul>"},{"location":"setup/devEnvironment/#using-development-containers-a-bit-more-setup-but-easier-long-term","title":"Using Development Containers (A bit more setup, but easier long term)","text":"<ul> <li>Install VSCode or your favorite IDE with development container support. These instructions will assume VSCode is used.</li> </ul>"},{"location":"setup/devEnvironment/#vscode","title":"VSCode","text":"<ul> <li>Ensure you have docker installed on your computer.</li> <li>Install the VSCode \"Dev Containers\" containers extension.</li> <li>Now you should be able to enter \"CTRL+SHIFT+P\", type in \"Dev Containers: Build and Open in Container\" </li> <li>This will use the existing .devcontainer.json to automatically setup the environment on your PC.</li> </ul>"},{"location":"setup/usingWebcamInContainer/","title":"Using Webcam in a Container","text":"<p>You may want to use a local webcam for the video processing or calibration.</p>"},{"location":"setup/usingWebcamInContainer/#connect-a-webcam-to-wsl-windows-machine","title":"Connect a Webcam to WSL (Windows Machine)","text":"<p>Since this project is expected to run in a Developer Container, the container must have access to a webcam to properly work.</p> <p>On windows the main way to run developer containers is to run them using WSL. Therefore the webcam must be accessible to your WSL instance. To do this, follow the instructions below. More details can be found on this webpage - Microsoft - Connect USB Devices to WSL</p> <ol> <li>List all USB devices connected to Windows by openning powershell in Admin mode and entering this command:   <code>usbipd list</code></li> <li>Before attaching the USB device, the command usbipd bind must be used to share the device, allowing it to be attached to WSL. Select the bus ID of the device you would like to use in WSL and run the following command:   <code>usbipd bind --busid 4-4</code></li> <li>To attach the USB device, run the following command. (You no longer need to use an elevated administrator prompt.) Ensure that a WSL command prompt is open in order to keep the WSL 2 lightweight VM active:    <code>usbipd attach --wsl --busid &lt;busid&gt;</code></li> <li>Open Ubuntu (or your preferred WSL command line) and list the attached USB devices using the command:   <code>lsusb</code></li> <li>Once you are done using the device in WSL, you can either physically disconnect the USB device or run this command from PowerShell:   <code>usbipd detach --busid &lt;busid&gt;</code></li> </ol>"},{"location":"setup/usingWebcamInContainer/#test-the-camera","title":"Test the Camera","text":"<p>You can test the camera is mounted to WSL and working by running this command <pre><code>apt-get install ffmpeg\nffmpeg -f v4l2 -framerate 1 -video_size 640x480 -i /dev/video0 -vframes 1 output_image.jpg\n</code></pre></p>"},{"location":"setup/usingWebcamInContainer/#mount-the-camera-to-the-container","title":"Mount the Camera to the Container","text":"<p>Mount the camera from your computer to video0 of the container to easily use it. Add the following line to your docker run command: <pre><code># Command: --device=&lt;path-to-video-device&gt;:/dev/video0\"\n# Example:\n--device=/dev/video0:/dev/video0\n</code></pre></p>"},{"location":"troubleshooting/cameraConnection/","title":"Investigating Camera Connectivity","text":"<p>Some instructions to investigate the connectivity issues with an attached camera in the docker container.</p>"},{"location":"troubleshooting/cameraConnection/#startup-container-with-following-commands","title":"Startup Container With Following Commands:","text":"<p>Startup the container, with the device mounted and the output folder mounted to look at output data. This assumes the image is called <code>tracking-image</code>, and the camera is attached to <code>/dev/video0</code>:</p> LinuxJetson5 <pre><code>docker run -it -v \"$(pwd)/output:/output\" --device=/dev/video0:/dev/video0 tracking-image \n</code></pre> <pre><code>docker run --ipc=host --runtime=nvidia -it -v \"$(pwd)/output:/output\" --device=/dev/video0:/dev/video0 tracking-image\n</code></pre>"},{"location":"troubleshooting/cameraConnection/#making-changes-then-rebuilding","title":"Making Changes Then Rebuilding","text":"<p>You can rebuild the container in between with these commands:</p> LinuxJetson5 <pre><code>docker build . -t tracking-image:latest\n</code></pre> <pre><code>docker build . -f Dockerfile-jetson-jetpack5 tracking-image:latest\n</code></pre>"},{"location":"troubleshooting/cameraConnection/#1-check-if-container-works-with-youtube-stream","title":"1 - Check If Container Works with Youtube Stream","text":"<p>First let's check the video processing works with a youtube stream. This will ensure that python code is working, and that the issue is with the local camera connection. The following command will skip radar processing as well, just to eliminate that factor as well. <pre><code>python3 tracking.py --skip-radar --skip-tracking --video-source \"https://youtu.be/LNwODJXcvt4\"\n</code></pre></p> <p>You should see processing output like this: <pre><code>1/1: https://youtu.be/LNwODJXcvt4... Success \u2705 (10842 frames of shape 1920x1080 at 30.00 FPS)\n\n0: 384x640 1 person, 1 potted plant, 53.0ms\n0: 384x640 1 person, 1 potted plant, 16.8ms\n0: 384x640 1 person, 1 potted plant, 26.1ms\n0: 384x640 1 person, 1 potted plant, 25.0ms\n0: 384x640 1 person, 1 potted plant, 16.1ms\n0: 384x640 1 person, 1 potted plant, 18.6ms\n0: 384x640 1 person, 1 potted plant, 20.2ms\n0: 384x640 1 person, 1 chair, 1 potted plant, 22.2ms\n0: 384x640 1 person, 1 chair, 1 potted plant, 17.4ms\n0: 384x640 1 person, 1 chair, 1 potted plant, 18.0ms\n0: 384x640 1 person, 1 chair, 1 potted plant, 20.8ms\n</code></pre></p>"},{"location":"troubleshooting/cameraConnection/#2-verify-device-access-with-ffmpeg","title":"2 - Verify Device Access With FFMPEG","text":"<p>Install and use FFMPEG inside the container as an initial check to ensure the camera is mounted properly. This output will be saved in the output folder assuming it's mounted to the container. <pre><code># Commands for running ffmpeg, install then capture frames\napt-get install ffmpeg\n\nffmpeg -f v4l2 -framerate 1 -video_size 640x480 -i /dev/video0 -vframes 1 /output/output_image.jpg\n</code></pre></p> <p>If that works, you should be able to see an image in the output folder.</p>"},{"location":"troubleshooting/cameraConnection/#3-check-if-yolo-works-with-the-device-through-cli","title":"3 - Check If Yolo Works With the Device Through CLI","text":"<p>Try to run the following command: <pre><code>yolo detect predict model=yolov8n.pt source=0\n</code></pre></p> <p>This will use the yolo cli to do a simple prediction. NOTE: Latest version of yolo only need the integer of the video source, not the entire video.</p>"},{"location":"troubleshooting/cameraConnection/#4-try-removing-stream-for-local-cameras","title":"4 - Try Removing Stream For Local Cameras","text":"<p>In the VideoConfiguration, change the videoStream to off and try again. This is unlikely to fix the issue, but worth trying.</p> <ul> <li> <p>Edit the config file in the pod, you can also mount a configuration directory to make this quicker. <pre><code># Install some text editor (nano, vi, etc)\nagt-get install nano\nnano /configuration/VideoConfiguration.yaml\n</code></pre></p> </li> <li> <p>Change the \"videoStream\" to False, the following setting: <pre><code># Configuration for the Yolo Processing\n...\niouThreshold: 0.5\nvideoStream: False # CHANGE THIS TO FALSE\n...\n</code></pre></p> </li> </ul> <p>Run the tracking program: <pre><code>python3 tracking.py --skip-radar --skip-tracking --video-source \"0\"\n</code></pre></p>"},{"location":"troubleshooting/cameraConnection/#other-alternatives","title":"Other Alternatives","text":"<p>There is a reported issue with using arducam's with the Orin Nano boards:</p> <ul> <li>Ultralytics Github Issue - Using Arducam (with gstreamer) and YoloV8</li> </ul> <p>There are some discussions there about using Nvidia-OpenCV rather than OpenCV that Ultralytics Yolo uses by default. If there are issues with using yolo with the Arducam, but it can be used with ffmpeg that switch may be needed.</p>"}]}